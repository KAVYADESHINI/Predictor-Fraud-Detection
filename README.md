ğŸ¥**Healthcare provider Fraud Detection System**

An end-to-end **Machine Learning project** to detect **fraudulent healthcare providers** by analyzing large-scale insurance claims data.
The system helps identify potential **Fraud, Waste, and Abuse** (FWA) using supervised learning models and is deployed as an interactive web application.

ğŸ”—**GitHub Repository**: https://github.com/KAVYADESHINI/Predictor-Fraud-Detection

ğŸ“Œ **Problem Statement**

Healthcare fraud leads to massive **financial losses** and reduced **trust** in healthcare systems.
The goal of this project is to **classify healthcare providers as fraudulent or non-fraudulent** by learning patterns from historical insurance claims data.

ğŸ“Š **Dataset Description**

The project uses public healthcare insurance claims data, integrating information from:
Provider details
Inpatient claims
Outpatient claims
Beneficiary data

ğŸ“ˆ **Scale:**

500,000+ insurance claim records
Highly imbalanced dataset (fraud cases are rare)

ğŸ§  **Approach & Methodology**
1ï¸âƒ£**Data Integration**
Merged multiple datasets using provider-level keys
Aggregated claims to create a provider-centric view

2ï¸âƒ£ **Feature Engineering**
Created meaningful features such as:
Total claim count per provider
Total reimbursement amount
Deductible and co-pay amounts
Inpatient vs outpatient claim ratios

3ï¸âƒ£ **Exploratory Data Analysis (EDA)**
Analyzed fraud vs non-fraud distributions
Identified key financial indicators correlated with fraud

4ï¸âƒ£ **Model Building**
Implemented and compared:
Logistic Regression
Random Forest Classifier

5ï¸âƒ£ **Model Evaluation**
Used fraud-sensitive metrics:
Precision
Recall
F1-score
ROC-AUC (Achieved **0.93**)
ğŸ”§ Threshold tuning was applied to improve fraud **recall** to **~75%***, prioritizing detection of fraudulent providers.

ğŸš€**Deployment**

Built an interactive Streamlit web application
Serialized trained model using Pickle
Enables real-time fraud risk prediction for providers

ğŸ› ï¸ **Tech Stack**
**Programming & Libraries**
Python
Pandas, NumPy
Scikit-learn
TensorFlow 
Streamlit
Machine Learning
Classification models
Feature engineering
Threshold tuning
Model evaluation (ROC-AUC, Confusion Matrix)

**Tools**
Git & GitHub
Pickle (model serialization)

ğŸ“‚ **Project Structure**
**Predictor-Fraud-Detection**/
â”‚
â”œâ”€â”€ data/                  # Raw and processed datasets
â”œâ”€â”€ notebooks/             # EDA and model development notebooks
â”œâ”€â”€ app.py                 # Streamlit application
â”œâ”€â”€ model.pkl              # Serialized trained model
â”œâ”€â”€ requirements.txt       # Project dependencies
â””â”€â”€ README.md              # Project documentation

ğŸ“ˆ **Key Results**
ROC-AUC Score: 0.93
Improved fraud recall to ~75%
Successfully deployed a usable ML application

ğŸŒŸ **Key Learnings**
Handling imbalanced datasets in real-world ML problems
Importance of feature engineering over model complexity
Selecting business-driven evaluation metrics
Deploying ML models for practical usage

ğŸ”® **Future Improvements**
Add advanced anomaly detection techniques
Experiment with ensemble and boosting models
Implement MLOps practices (CI/CD, model monitoring)
Improve UI and add explainability (SHAP values)

ğŸ‘©â€ğŸ’»**Author**
Kavya Deshini
Entry-Level Machine Learning Engineer
ğŸ“§ Email: kavyadeshini1224@gmail.com

ğŸ”— GitHub: https://github.com/KAVYADESHINI
